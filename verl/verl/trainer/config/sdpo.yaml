defaults:
  - ppo_trainer
  - user_runpod  # RunPod user configuration
  - _self_

# MAX_PROMPT_LENGTH=2048
# MAX_RESPONSE_LENGTH=8192
# MAX_FEEDBACK_LENGTH=8192
# TEMPLATE_LENGTH=512  # heuristic upper bound, not enforced
# MAX_MODEL_LEN=$((TEMPLATE_LENGTH + MAX_PROMPT_LENGTH + MAX_FEEDBACK_LENGTH + MAX_RESPONSE_LENGTH))
max_model_len: 18944

actor_rollout_ref:
  actor:
    ppo_mini_batch_size: 32 # TODO: make slightly off-policy in the future; use same as baseline; perhaps make 1 again for fastest convergence
    policy_loss:
      loss_mode: sdpo
    self_distillation:
      # Distillation settings (matching SDPO experiments/rich_feedback/run_sdpo.sh)
      alpha: 1.0  # reverse KL
      full_logit_distillation: true
      distillation_topk: 20  # experiments use top-20, not default 100
      distillation_add_tail: true
      teacher_update_rate: 0.01  # EMA rate from experiments
      # Reprompting settings
      max_reprompt_len: 10240
      is_clip: 2.0
      success_reward_threshold: 1.0
      include_environment_feedback: true
      dont_reprompt_on_self_success: true
      remove_thinking_from_demonstration: true  # Remove <think>...</think> from demonstrations
      environment_feedback_only_without_solution: true  # Only use feedback when no solution available
      # Reprompting template - combines prompt, solution section, and feedback section
      reprompt_template: |-
        {prompt}{solution}{feedback}

        Correctly solve the original question.
      # Solution template - formats successful previous attempts
      solution_template: |-

        Correct solution:

        {successful_previous_attempt}

      # Feedback template - formats environment feedback from failed attempts
      feedback_template: |-

        The following is feedback from your unsuccessful earlier attempt:

        {feedback_raw}
    optim:
      lr: 1e-5
  rollout:
    n: 8
    calculate_log_probs: True

algorithm:
  adv_estimator: grpo # disables critic
  norm_adv_by_std_in_grpo: False
  rollout_correction:
    rollout_is: token
    rollout_is_threshold: 2.0

data:
  train_batch_size: 32

trainer:
  val_before_train: False
