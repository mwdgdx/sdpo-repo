defaults:
  - ppo_trainer
  - user_runpod  # RunPod user configuration
  - _self_

# MAX_PROMPT_LENGTH=2048
# MAX_RESPONSE_LENGTH=4096
# Reduced max_model_len for faster vLLM initialization
max_model_len: 8192

actor_rollout_ref:
  actor:
    ppo_mini_batch_size: 32 # TODO: make slightly off-policy in the future; use same as baseline; perhaps make 1 again for fastest convergence
    policy_loss:
      loss_mode: sdpo
    self_distillation:
      max_reprompt_len: 6144
      is_clip: 2.0
    optim:
      lr: 1e-5
  rollout:
    n: 8
    calculate_log_probs: True

algorithm:
  adv_estimator: grpo # disables critic
  norm_adv_by_std_in_grpo: False
  rollout_correction:
    rollout_is: token
    rollout_is_threshold: 2.0

data:
  train_batch_size: 32

trainer:
  val_before_train: False
